{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3d011d",
   "metadata": {},
   "source": [
    "## --- Generate text with a recurrent neural network (Pytorch) ---\n",
    "### (Mostly Read & Run)\n",
    "\n",
    "The goal is to replicate the (famous) experiment from [Karpathy's blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "To learn to generate text, we train a recurrent neural network to do the following task:\n",
    "\n",
    "Given a \"chunk\" of text: `this is random text`\n",
    "\n",
    "the goal of the network is to predict each character in **`his is random text` ** sequentially given the following sequential input **`this is random tex`**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "905e7491",
   "metadata": {},
   "source": [
    "Input ->  Output\n",
    "--------------\n",
    "T    ->    H\n",
    "H    ->    I\n",
    "I    ->    S\n",
    "S    ->    \" \"\n",
    "\" \"  ->    I\n",
    "I    ->    S\n",
    "S    ->    \" \"\n",
    "[...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb3f04",
   "metadata": {},
   "source": [
    "\n",
    "## Load text (dataset/input.txt)\n",
    "\n",
    "Before building training batch, we load the full text in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d03d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-03-01 14:47:53--  https://thome.isir.upmc.fr/classes/RITAL/input.txt\n",
      "Resolving thome.isir.upmc.fr (thome.isir.upmc.fr)... 134.157.18.247\n",
      "Connecting to thome.isir.upmc.fr (thome.isir.upmc.fr)|134.157.18.247|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1,1M) [text/plain]\n",
      "Saving to: 'input.txt.1'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  4% 2,08M 0s\n",
      "    50K .......... .......... .......... .......... ..........  9% 3,03M 0s\n",
      "   100K .......... .......... .......... .......... .......... 13% 7,04M 0s\n",
      "   150K .......... .......... .......... .......... .......... 18% 2,08M 0s\n",
      "   200K .......... .......... .......... .......... .......... 22% 44,4M 0s\n",
      "   250K .......... .......... .......... .......... .......... 27% 6,11M 0s\n",
      "   300K .......... .......... .......... .......... .......... 32% 4,98M 0s\n",
      "   350K .......... .......... .......... .......... .......... 36%  584K 0s\n",
      "   400K .......... .......... .......... .......... .......... 41% 3,99M 0s\n",
      "   450K .......... .......... .......... .......... .......... 45% 5,78M 0s\n",
      "   500K .......... .......... .......... .......... .......... 50% 4,37M 0s\n",
      "   550K .......... .......... .......... .......... .......... 55% 4,91M 0s\n",
      "   600K .......... .......... .......... .......... .......... 59% 4,39M 0s\n",
      "   650K .......... .......... .......... .......... .......... 64% 4,72M 0s\n",
      "   700K .......... .......... .......... .......... .......... 68% 4,44M 0s\n",
      "   750K .......... .......... .......... .......... .......... 73% 3,14M 0s\n",
      "   800K .......... .......... .......... .......... .......... 78% 3,80M 0s\n",
      "   850K .......... .......... .......... .......... .......... 82% 2,87M 0s\n",
      "   900K .......... .......... .......... .......... .......... 87% 3,02M 0s\n",
      "   950K .......... .......... .......... .......... .......... 91% 3,64M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 96% 2,00M 0s\n",
      "  1050K .......... .......... .......... .........            100% 5,88M=0,4s\n",
      "\n",
      "2023-03-01 14:47:54 (3,00 MB/s) - 'input.txt.1' saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://thome.isir.upmc.fr/classes/RITAL/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa52e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "890c249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = unidecode.unidecode(open('./input.txt').read()) #clean text => only ascii\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af54275",
   "metadata": {},
   "source": [
    "## 2: Helper functions:\n",
    "\n",
    "We have a text and we want to feed batch of chunks to a neural network:\n",
    "\n",
    "one chunk  A,B,C,D,E\n",
    "[input] A,B,C,D -> B,C,D,E [output]\n",
    "\n",
    "Note: we will use an embedding layer instead of a one-hot encoding scheme.\n",
    "\n",
    "for this, we have 3 functions:\n",
    "\n",
    "- One to get a random str chunk of size `chunk_len` : `random_chunk` \n",
    "- One to turn a chunk into a tensor of size `(1,chunk_len)` coding for each characters : `char_tensor`\n",
    "- One to return random input and output chunks of size `(batch_size,chunk_len)` : `random_training_set`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f68d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[44, 68, 21, 21, 94, 29, 10, 27, 27, 34],\n",
      "        [28, 73, 94, 32, 17, 10, 29, 94, 23, 14],\n",
      "        [34, 94, 17, 14, 94, 11, 14, 94, 10, 29],\n",
      "        [32, 10, 28, 94, 10, 11, 24, 30, 29, 94]]), tensor([[68, 21, 21, 94, 29, 10, 27, 27, 34, 94],\n",
      "        [73, 94, 32, 17, 10, 29, 94, 23, 14, 32],\n",
      "        [94, 17, 14, 94, 11, 14, 94, 10, 29, 94],\n",
      "        [10, 28, 94, 10, 11, 24, 30, 29, 94, 29]]))\n"
     ]
    }
   ],
   "source": [
    "import time, math\n",
    "\n",
    "\n",
    "#Get a piece of text\n",
    "def random_chunk(chunk_len):\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "\n",
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(1,len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[0,c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "\n",
    "#Turn a piece of text in train/test\n",
    "def random_training_set(chunk_len=200, batch_size=8):\n",
    "    chunks = [random_chunk(chunk_len) for _ in range(batch_size)]\n",
    "    inp = torch.cat([char_tensor(chunk[:-1]) for chunk in chunks],dim=0)\n",
    "    target = torch.cat([char_tensor(chunk[1:]) for chunk in chunks],dim=0)\n",
    "    \n",
    "    return inp, target\n",
    "\n",
    "print(random_training_set(10,4))  ## should return 8 chunks of 10 letters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141dad88",
   "metadata": {},
   "source": [
    "## The actual RNN model (only thing to complete):\n",
    "\n",
    "It should be composed of three distinct modules:\n",
    "\n",
    "- an [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) (n_characters, hidden_size)\n",
    "\n",
    "```\n",
    "nn.Embedding(len_dic,size_vec)\n",
    "```\n",
    "- a [recurrent](https://pytorch.org/docs/stable/nn.html#recurrent-layers) layer (hidden_size, hidden_size)\n",
    "```\n",
    "nn.RNN(in_size,out_size) or nn.GRU() or nn.LSTM() => rnn_cell parameter\n",
    "```\n",
    "- a [prediction](https://pytorch.org/docs/stable/nn.html#linear) layer (hidden_size, output_size)\n",
    "\n",
    "```\n",
    "nn.Linear(in_size,out_size)\n",
    "```\n",
    "=> Complete the `init` function code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d838e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_char, hidden_size, output_size, n_layers=1,rnn_cell=nn.RNN):\n",
    "        \"\"\"\n",
    "        Create the network\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.n_char = n_char\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #  (batch,chunk_len) -> (batch, chunk_len, hidden_size)  \n",
    "        self.embed = nn.Embedding(n_char,hidden_size)\n",
    "        \n",
    "        # (batch, chunk_len, hidden_size)  -> (batch, chunk_len, hidden_size)  \n",
    "        self.rnn = rnn_cell(hidden_size,hidden_size,n_layers)\n",
    "        \n",
    "        #(batch, chunk_len, hidden_size) -> (batch, chunk_len, output_size)  \n",
    "        self.predict = nn.Linear(hidden_size,output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        batched forward: input is (batch > 1,chunk_len)\n",
    "        \"\"\"\n",
    "        input = self.embed(input)\n",
    "        output,_  = self.rnn(input)\n",
    "        output = self.predict(f.tanh(output))\n",
    "        return output\n",
    "    \n",
    "    def forward_seq(self, input,hidden=None):\n",
    "        \"\"\"\n",
    "        not batched forward: input is  (1,chunk_len)\n",
    "        \"\"\"\n",
    "        input = self.embed(input)\n",
    "        output,hidden  = self.rnn(input.unsqueeze(0),hidden)\n",
    "        output = self.predict(f.tanh(output))\n",
    "        return output,hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34643b32",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Text generation function\n",
    "\n",
    "Sample text from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c751d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,prime_str='A', predict_len=100, temperature=0.8):\n",
    "    prime_input = char_tensor(prime_str).squeeze(0)\n",
    "    hidden = None\n",
    "    predicted = prime_str+\"\"\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "\n",
    "    for p in range(len(prime_str)-1):\n",
    "        _,hidden = model.forward_seq(prime_input[p].unsqueeze(0),hidden)\n",
    "            \n",
    "    #print(hidden.size())\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model.forward_seq(prime_input[-1].unsqueeze(0), hidden)\n",
    "                # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        #print(output_dist)\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        #print(top_i)\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        prime_input = torch.cat([prime_input,char_tensor(predicted_char).squeeze(0)])\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8783b21d",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training loop for net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9acbf3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 14s (100 0%) 3.0874]\n",
      "WhaI tOre Te myheleedero h ter f thru t.-sae\n",
      "sea\n",
      "moEhhap botedtase.reloan Bs theoG tosnlan u dat, n  t \n",
      "\n",
      "[0m 29s (200 1%) 2.9836]\n",
      "Wh ase pl era I s.\n",
      "Rt Iatentoran\n",
      "soton\n",
      "hl m loom y an lo? mede Uf ch mtan an n thaApusp arstr h ses I\n",
      " \n",
      "\n",
      "[0m 44s (300 1%) 2.6487]\n",
      "Wh r the me re?\n",
      "Mleanedon.\n",
      "A y t.\n",
      "I me gerlot mecon n;\n",
      "Is ru he ele th tke ithetil f asob giitheson th \n",
      "\n",
      "[0m 59s (400 2%) 2.5859]\n",
      "Whe t ar t homes or s, od RCis le he-crthon sl VIororlenced\n",
      "\n",
      "Batorsow cle we athed ond,\n",
      "Y:\n",
      "Wotowthoro  \n",
      "\n",
      "[1m 12s (500 2%) 2.6472]\n",
      "Whe his as ofolisln ancot ponou me ROS todsis tr, met bimere ltory he pe as macres in lymare d t misot \n",
      "\n",
      "[1m 26s (600 3%) 2.5805]\n",
      "Whs y he.\n",
      "E: it aknlorve, a hy w.\n",
      "\n",
      "OThe d t ticuk hitite ans e ilely w e ur ile:\n",
      "Anodillr has isthe th \n",
      "\n",
      "[1m 41s (700 3%) 2.5409]\n",
      "Whe tas k be f d tor he tod the ithofe ainonco'd lloud d, buthenowandind be t!\n",
      "CENG: this digcas,\n",
      "A ba \n",
      "\n",
      "[1m 55s (800 4%) 2.5448]\n",
      "Wh.\n",
      "\n",
      "\n",
      "NI\n",
      "OHeripl an car thesoue ono we mesingicee I I b ds ard nd tilete n as'sengharealle PENUENOSes, \n",
      "\n",
      "[2m 10s (900 4%) 2.5301]\n",
      "Whe's do her s, pinte theouthoos crst t win drind enee chale,\n",
      "Camor cetro wth.\n",
      "An fs, ar he iro sthout \n",
      "\n",
      "[2m 24s (1000 5%) 2.5259]\n",
      "Whe ge d ir thewe heend y sand in we tor turul g nthand y as.\n",
      "O:\n",
      "\n",
      "Ba wod avind, she sotinour wis'lend  \n",
      "\n",
      "[2m 39s (1100 5%) 2.5311]\n",
      "Whepondo the shes s whan h t ANas h yad, nes whee, ar't athanll y IUS: an ous who at d h s, My y f whe \n",
      "\n",
      "[2m 53s (1200 6%) 2.4904]\n",
      "Whango arer hed m thethid me r ithenss t t buntere we gavecemeshele f heped tbe ben id tither, d thand \n",
      "\n",
      "[3m 7s (1300 6%) 2.5492]\n",
      "Whe aneremy pr twint;\n",
      "OKE liches\n",
      "\n",
      "MRY:\n",
      "\n",
      "TERYO:\n",
      "A mlalin me th Thethe plins foth por and rr'thane wik t \n",
      "\n",
      "[3m 22s (1400 7%) 2.5477]\n",
      "Wharipak.\n",
      "\n",
      "AULO:\n",
      "\n",
      "Whor's fou limatour k'sthe s ts ay akee it tererumy mf ameang wes mond malink cos me \n",
      "\n",
      "[3m 37s (1500 7%) 2.5165]\n",
      "Whare lathaigrerd ist w s way mi, y y blese t peavong she Cous athe, fe MI noures hy yor watcoriler ar \n",
      "\n",
      "[3m 51s (1600 8%) 2.4878]\n",
      "Wheda o an he d. heme thadle bllle\n",
      "By ndallincodar ain' d:\n",
      "Y bee.\n",
      "NCY:\n",
      "IOREY wincle,\n",
      "\n",
      "Fiomaseh; rour o \n",
      "\n",
      "[4m 4s (1700 8%) 2.4914]\n",
      "Whe l w y, atere is he tho kerthitindoutone myou d thononery Itsaveano gero, mive gagr ar d irok merdi \n",
      "\n",
      "[4m 19s (1800 9%) 2.4502]\n",
      "Whe by and llla f whenyond by s py al wis ave boweroushalleiretely besas tthe s it, o tourave ot wofer \n",
      "\n",
      "[4m 33s (1900 9%) 2.4919]\n",
      "Wh piselllis s.\n",
      "Wom ofde he y pe t ghadrelld me pyoor y arsth m tt?\n",
      "\n",
      "\n",
      "Thofore? ad hee lino to pe penis \n",
      "\n",
      "[4m 48s (2000 10%) 2.5452]\n",
      "Whathar manthis in my my s f arof, tin y s it pere thamonthathe t beryath st my thind imy she bereaich \n",
      "\n",
      "[5m 2s (2100 10%) 2.5119]\n",
      "Whe weve t, mur s ch t his,\n",
      "Wis,\n",
      "\n",
      "\n",
      "KUCs Jowice y d witre d ing be hicisealit mand at,\n",
      "CENG fant so f a \n",
      "\n",
      "[5m 17s (2200 11%) 2.5643]\n",
      "Whe. sth insu stll bthasitc:\n",
      "\n",
      "PAne mett trathenom t bell, ather t ORDUCl arad I lled CIZ:\n",
      "Wer ll\n",
      "The\n",
      "\n",
      " \n",
      "\n",
      "[5m 31s (2300 11%) 2.4326]\n",
      "Whouthead\n",
      "I XENUSrendoy we t ely gu sou thindove at w ite bet s n.\n",
      "\n",
      "MI:\n",
      "\n",
      "HEThill t w\n",
      "NGA:\n",
      "\n",
      "BRTove pr n \n",
      "\n",
      "[5m 45s (2400 12%) 2.4863]\n",
      "Whe hinthathof coughathit list wiver fetou arsope wousopt t as ifis pthowerd tousange lan ingher housi \n",
      "\n",
      "[6m 0s (2500 12%) 2.4347]\n",
      "Whener.\n",
      "Thithay t an st me owisof--d,\n",
      "IUCENULAD p dind ard, andatrisitarnce cino m s d ore, GLAnt that \n",
      "\n",
      "[6m 13s (2600 13%) 2.4453]\n",
      "Why hy f d he,\n",
      "\n",
      "Cld owais fomar'doulous bere the hon it or eid?\n",
      "Tot\n",
      "An sesourt y thed-sea hie ous we w \n",
      "\n",
      "[6m 28s (2700 13%) 2.5461]\n",
      "Why k;\n",
      "Whathet lithomyotond iutharonsl KI:\n",
      "\n",
      "Wo allis imurealy ban tet, mrsour, ret ted, he wite ner y  \n",
      "\n",
      "[6m 44s (2800 14%) 2.4697]\n",
      "Whe s, o kend d ckee le ar he bunce d hengas an myoureatithe sim wes orfot mofon G thindie ay veds mot \n",
      "\n",
      "[6m 59s (2900 14%) 2.5301]\n",
      "Wher, EMENore tar pan ot hirayowatoure inthe s.\n",
      "Thagoumyooors n\n",
      "ange me!\n",
      "I reangor thare in.\n",
      "Whick nep \n",
      "\n",
      "[7m 14s (3000 15%) 2.5001]\n",
      "Whighay ay bl pera,\n",
      "Fome chas tes g tethe athewengeves h akigelimeth dis wangewolifond He nd s oure se \n",
      "\n",
      "[7m 30s (3100 15%) 2.4944]\n",
      "Whe atoone t n e wixu the y youe ty.\n",
      "IONESendikeriok ier m f tereant s Fo oo,\n",
      "Pa kn ath badarieis ad;  \n",
      "\n",
      "[7m 45s (3200 16%) 2.5593]\n",
      "Whe h alloular an this buitoo moupe there myu g cee aioren rl ir lerd an as be mer ch d y h indo bllon \n",
      "\n",
      "[8m 1s (3300 16%) 2.5675]\n",
      "Whonowoun l toon, ourie, shes four irof der, fousond tse parthinthathaico shin pe sor weale tot ore he \n",
      "\n",
      "[8m 16s (3400 17%) 2.4795]\n",
      "Whorre tr lloulanolico he s ne PRKowe th asod heroupes cr wir t.\n",
      "YO:\n",
      "Thes me ting s th at ng, waplavel \n",
      "\n",
      "[8m 31s (3500 17%) 2.5284]\n",
      "Wherifar h s, t sie n br war be. n,\n",
      "\n",
      "\n",
      "The cr men is he-moul mid me patherend,\n",
      "I thast wer-gha of her,  \n",
      "\n",
      "[8m 46s (3600 18%) 2.5559]\n",
      "Whasr, has hal an t cowhs o sheakno bld carayoroou I andirime helenndong nshe thed bo mot g be the far \n",
      "\n",
      "[9m 0s (3700 18%) 2.5263]\n",
      "Whyom thaidout n, bes\n",
      "ARD:\n",
      "K:\n",
      "\n",
      "\n",
      "ST: dour wo me,\n",
      "Thitrn s, r s t areanoultord s tond sere ien fan I IOM \n",
      "\n",
      "[9m 15s (3800 19%) 2.5200]\n",
      "Whe nonourow pr seat weat mo shit:\n",
      "O:\n",
      "To wind abe the attour fano sthantotit betiesiniglld br ime imor \n",
      "\n",
      "[9m 30s (3900 19%) 2.4414]\n",
      "Whise an I y t herd thorse t o he me omom th\n",
      "\n",
      "Ang meso d your maret S:\n",
      "Thinof th w y s le y, an hielli \n",
      "\n",
      "[9m 45s (4000 20%) 2.5569]\n",
      "Wh t IOnthemy g tr thorthe Intanothend, bus hapathes o nouss this le cele can tay we yot mit henellerc \n",
      "\n",
      "[10m 1s (4100 20%) 2.4806]\n",
      "Wheat t r ncherill an n tor.\n",
      "Wher d cess it, po; an, f\n",
      "ARCA:\n",
      "NTE:\n",
      "\n",
      "T:\n",
      "S:\n",
      "\n",
      "CHoorperen nghe, ainthendge  \n",
      "\n",
      "[10m 15s (4200 21%) 2.4966]\n",
      "Whe hereofe sould the t inind tcord t fender f l chee G atorony me mire thear; ticepr E:\n",
      "S:\n",
      "\n",
      "Whirsthes \n",
      "\n",
      "[10m 28s (4300 21%) 2.4829]\n",
      "Whyoupe cknor d ha vethea tyon in thameng towengherro ansay te RDousar, yo sthaulinou ave, berendwisso \n",
      "\n",
      "[10m 40s (4400 22%) 2.5181]\n",
      "When folond furs gre lie t le inthe Cloralane hing ericrime t ild t,\n",
      "\n",
      "ER:\n",
      "The y lis;\n",
      "Mallel ak on stis \n",
      "\n",
      "[10m 53s (4500 22%) 2.4586]\n",
      "Whouthenthyit bethice wore eshilles hingonfit a averig annosourst\n",
      "Bonces r than sthillan nd n;\n",
      "O:\n",
      "NCES \n",
      "\n",
      "[11m 5s (4600 23%) 2.4986]\n",
      "Whe m swiss watche thars t in aiplle te neiou wis:\n",
      "Hive ws sthiler sus jowiput mind arar t:\n",
      "\n",
      "ES ir,\n",
      "\n",
      "T \n",
      "\n",
      "[11m 19s (4700 23%) 2.5381]\n",
      "Whe thy athe, be y sey nkernd he her rs tuie?\n",
      "Sre.\n",
      "Y he s T:\n",
      "AMERUTonerd pe se saner hel atowhe, makis \n",
      "\n",
      "[11m 34s (4800 24%) 2.4935]\n",
      "Wh mast f pe thor,\n",
      "D thomowiorr bomance ompest l te pay at hat\n",
      "A my.\n",
      "BII twrs?\n",
      "Thidelime qus ghad h oo \n",
      "\n",
      "[11m 49s (4900 24%) 2.4642]\n",
      "Whyo sgnoo here s's rd nd pray ltid hin CHINon, t ind ce g,\n",
      "INCE: wh on winererfe be bere t venofave d \n",
      "\n",
      "[12m 5s (5000 25%) 2.5029]\n",
      "Where fon hens ll y ar weriapaure s Y louralle char ous m.\n",
      "\n",
      "SThe y ce t t?\n",
      "Prend g, wisitowolo, can tt \n",
      "\n",
      "[12m 20s (5100 25%) 2.5000]\n",
      "Wher sheacksiniscredrmat:\n",
      "Then s.\n",
      "We s mthof out hilechar mueeseper'd wacuchithang t hore f\n",
      "I wica bun \n",
      "\n",
      "[12m 35s (5200 26%) 2.4890]\n",
      "Whenooupo ceano t hind s thed ste, otit tis we tore s,\n",
      "\n",
      "G I wh ccavel!\n",
      "INThe t ck?\n",
      "\n",
      "\n",
      "Thep vinow; n t s \n",
      "\n",
      "[12m 51s (5300 26%) 2.4916]\n",
      "Whanthe ure w lllorarithathatin byo swan.\n",
      "STETha the d ang se t whathoughend lo f at ile haberd loeang \n",
      "\n",
      "[13m 5s (5400 27%) 2.4246]\n",
      "Wh bas ithethe, in g'segis thatcly t whinore!\n",
      "OS:\n",
      "\n",
      "Too, y t loff we weq ce h hainere\n",
      "\n",
      "Tor PO:\n",
      "HELORout \n",
      "\n",
      "[13m 21s (5500 27%) 2.4299]\n",
      "Wh tl s it athit ose Y d IUSRY:\n",
      "TROungis arallee been g ur cche, Bupr thu y oisere brsulltho lathot ss \n",
      "\n",
      "[13m 35s (5600 28%) 2.4704]\n",
      "Whicoomy they hthes r yoree athaingale he,\n",
      "Da\n",
      "ILano loppenchithe wikn tise, d; faghe, sean hean ir wha \n",
      "\n",
      "[13m 50s (5700 28%) 2.4696]\n",
      "Whelisere arvorchealar tofee maryog Thera, he t y, fth wivexthet as ogort woulo, er d the omashe t ppo \n",
      "\n",
      "[14m 4s (5800 28%) 2.5242]\n",
      "Whistith s.\n",
      "Can sour me t rthitoneano tr mit m'd f thand n minges ousto AR:\n",
      "TUELo.\n",
      "\n",
      "I thet th ise t bo \n",
      "\n",
      "[14m 17s (5900 29%) 2.4489]\n",
      "Whist d ase ERD:\n",
      "Whel te owhe ingocorro keres t che sh othis itouin s thoil be f s yor 'ds r MI t bupr \n",
      "\n",
      "[14m 31s (6000 30%) 2.4374]\n",
      "Whad, Ho es whe choure mens nd, ille m buganer rt f nd hrist thy D tiglo hind the t ARDUNo I hoo.\n",
      "HAn  \n",
      "\n",
      "[14m 44s (6100 30%) 2.4210]\n",
      "Whe t I sthand the se be, te h, fouseando ENThy, womy by thale bed.\n",
      "Th, thy LAshe le st sefugr whe nd  \n",
      "\n",
      "[14m 58s (6200 31%) 2.5118]\n",
      "Whath har hesesesthany ardane cho s y s mad ore:\n",
      "MENou aind hatherday bles htod m s, yoll ostr d myo h \n",
      "\n",
      "[15m 11s (6300 31%) 2.4810]\n",
      "Whand he ad s d stherarurat me, se.\n",
      "Wigre ngrinore memourth f s r y and;\n",
      "Gre\n",
      "Koulo psero anoumyousous, \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15m 25s (6400 32%) 2.4668]\n",
      "Whon Hent s this s nd awe,-ge ittheresemen he mad tathe se thinthin ing.\n",
      "Wee thatenge t a art ar, my t \n",
      "\n",
      "[15m 39s (6500 32%) 2.3905]\n",
      "Whed, e pese,\n",
      "Therd,\n",
      "Gthe itcos! inoves her ongoul:\n",
      "Thar! ing co anomees, pu ane ghofosthe's nd th gad \n",
      "\n",
      "[15m 52s (6600 33%) 2.4382]\n",
      "Whetr ithy br bematherendsut than the my t blin man ayoulisuradealot, s h feno t hote thorerelo douse  \n",
      "\n",
      "[16m 5s (6700 33%) 2.4527]\n",
      "Whenounod my, f arinone ht al as imyomemen whe froucer s ous the'd oug dathe wo tht ifo m llf thirchal \n",
      "\n",
      "[16m 18s (6800 34%) 2.4446]\n",
      "Whert wrensone jent owin llowou thino,\n",
      "NNThise\n",
      "\n",
      "Ans, itrouth g moucon bed s's.\n",
      "O:\n",
      "ORERoroucastheed t h \n",
      "\n",
      "[16m 31s (6900 34%) 2.4292]\n",
      "Whencakeare t toupearing's here. ves hourd n.\n",
      "Sh ourd an seroo on t f s thensthanthe beee.\n",
      "HI ll we Bu \n",
      "\n",
      "[16m 45s (7000 35%) 2.4690]\n",
      "Wher. s f ghel!\n",
      "\n",
      "IULOfar LO:\n",
      "PENu dd theas roumy,\n",
      "HENGouldere tinelay iowere mas t omy andof rlaiee, h \n",
      "\n",
      "[16m 58s (7100 35%) 2.4532]\n",
      "Whaingheng;\n",
      "\n",
      "\n",
      "LINENCORI tresirstouchellls,\n",
      "Mathathyde be theay O:\n",
      "\n",
      "\n",
      "To dour w I s INI Thangondas bory  \n",
      "\n",
      "[17m 11s (7200 36%) 2.4493]\n",
      "Whothth sprt the y pesheyoud h sthather s m, t.\n",
      "\n",
      "BANowhy wid was it them blltedirese y ico,\n",
      "Yeverkend  \n",
      "\n",
      "[17m 25s (7300 36%) 2.5130]\n",
      "Whisthaps me som, bllldit, hated ad y att warese memefige atou Fany y,\n",
      "Mare serd alenin're weipak bema \n",
      "\n",
      "[17m 38s (7400 37%) 2.4896]\n",
      "Whe\n",
      "ANonor sesoupe, me oue beer s he s led s oure stte meewithath s h id, pee hor ousu scanthy shinoro \n",
      "\n",
      "[17m 52s (7500 37%) 2.4470]\n",
      "Whous I:\n",
      "ABO:\n",
      "Whangesl as I sou be fathat p.\n",
      "A:\n",
      "\n",
      "ATnd l ingo bjerind.\n",
      "\n",
      "\n",
      "Pran we l f thido wis the n an \n",
      "\n",
      "[18m 5s (7600 38%) 2.4793]\n",
      "Who fofor fe s ant in us me' me,\n",
      "RDY myng.\n",
      "LLLAnd wn' pit ler tilthoo RIESe thot anoud\n",
      "DWhe f he,\n",
      "\n",
      "Whe \n",
      "\n",
      "[18m 18s (7700 38%) 2.4959]\n",
      "Whiclou\n",
      "S: hes:\n",
      "Thy y the pe towis sond f ber ied izer t acchendo it me.\n",
      "IOFor fis I s pr's\n",
      "I thesyour \n",
      "\n",
      "[18m 32s (7800 39%) 2.4746]\n",
      "Wh, inid s,\n",
      "Wind th teruse TIORDI d, fd s, at, ng s EROt'd t hitowencio a maiteren Is t cke cowan feay \n",
      "\n",
      "[18m 45s (7900 39%) 2.4304]\n",
      "Wh g s t ther thar, thout didom, averanor ticithe oner CA:\n",
      "Ang me s wis tthour wind win olodsthoupit W \n",
      "\n",
      "[18m 59s (8000 40%) 2.5206]\n",
      "Whout th mourd o he foberes ho I he tanitheve forlofener win palld s hathe\n",
      "Whe prout anonendmyof ave m \n",
      "\n",
      "[19m 12s (8100 40%) 2.5049]\n",
      "Whunongrethathe thene t k.\n",
      "\n",
      "\n",
      "HO:\n",
      "Thers w BEROLICES:\n",
      "INUSangores andieodvese ho mir, th we me ak the it \n",
      "\n",
      "[19m 26s (8200 41%) 2.4636]\n",
      "Whakipemoourtbacongouth wim.\n",
      "DUCLOMared han thindover wird, int henthind s he leruriarin d arnd Ifad,  \n",
      "\n",
      "[19m 39s (8300 41%) 2.4816]\n",
      "Wh, y winged ho roree my .\n",
      "The w! awow th m maiere we wnt the t t,' kinon n, hererdove itrby y l, me n \n",
      "\n",
      "[19m 53s (8400 42%) 2.5196]\n",
      "Whisat than, t r st thatond t inda oreere oure RDre boulead of nd thofe sttatoflitifig arf r hy FO:\n",
      "\n",
      "P \n",
      "\n",
      "[20m 7s (8500 42%) 2.4960]\n",
      "Whot oromy t rat the ate han inoot thin brir'dof mand g minchese l, ee olote thin irofoo thoveathecole \n",
      "\n",
      "[20m 21s (8600 43%) 2.4389]\n",
      "Whersthie theanon aps sa d wes th orerstu fined o ino s, f derur po t l pe t whe ly the thie ngines ch \n",
      "\n",
      "[20m 35s (8700 43%) 2.4470]\n",
      "Whe w r, he gond y thisous t chet thave t ned pprenghameanthas I o pu I bue thinthee d thedo he hand m \n",
      "\n",
      "[20m 49s (8800 44%) 2.4095]\n",
      "Why wound as:\n",
      "GLUDon hir'll bas, n cer hithin luron un aurlllal hor aneare de haime.\n",
      "HIO:\n",
      "S:\n",
      "Hace four \n",
      "\n",
      "[21m 2s (8900 44%) 2.4705]\n",
      "Whounjof baro pou.\n",
      "\n",
      "\n",
      "We hed dl anco tht my, ce omy lyon ullarde s s tincove ARYAR:\n",
      "\n",
      "\n",
      "I he ck barthe se \n",
      "\n",
      "[21m 15s (9000 45%) 2.4892]\n",
      "Whish w mithe, as n, beth RIOUKI thay s the an tbutrer mathe m thind INu ere thy s.\n",
      "\n",
      "POLO:\n",
      "\n",
      "I IO:\n",
      "\n",
      "Mal \n",
      "\n",
      "[21m 29s (9100 45%) 2.4657]\n",
      "Whitou ys hele l, wis chavathin bor vermy be fo tou he I s ce. mord m, herer t tefoun, hintost quther  \n",
      "\n",
      "[21m 43s (9200 46%) 2.4332]\n",
      "Whe WIOf athelor alfu y atis ckneea ak ss f s nd be the fre and quro ty I yody be I athe.\n",
      "Le I te thar \n",
      "\n",
      "[21m 56s (9300 46%) 2.4064]\n",
      "Whe be st arr r, bromanghifamige mothathe thin whon t s suron r pllll harat I ir tean lenge mecero ag  \n",
      "\n",
      "[22m 10s (9400 47%) 2.4598]\n",
      "Whesaldo be,\n",
      "Ton badougelou g he I t amealo ceay itrertliser more ou, aren?\n",
      "SO whered pelinnd, s!\n",
      "Yomy \n",
      "\n",
      "[22m 23s (9500 47%) 2.5029]\n",
      "Whe maroreralersheswoford seay hounound h ENon! p CEN:\n",
      "NI ar, tchar br d maloveay thome med th f\n",
      "Touru \n",
      "\n",
      "[22m 36s (9600 48%) 2.5097]\n",
      "What l'lid aden.\n",
      "S:\n",
      "Wiges INGHo ce in mathe athes he o sere ano witif t! he byono f s he'sust he Y wit \n",
      "\n",
      "[22m 50s (9700 48%) 2.4569]\n",
      "Where arith.\n",
      "Howar wesersharr INurinoball.\n",
      "O:\n",
      "\n",
      "OLONO:\n",
      "\n",
      "BEORo me thowhe w the fend he y Br bsar, anoudo \n",
      "\n",
      "[23m 3s (9800 49%) 2.5076]\n",
      "Whon te haure shas whye angeder;\n",
      "An y nonde cit thay ay For te, he m.\n",
      "ABe, frineseeine lathay h a beav \n",
      "\n",
      "[23m 17s (9900 49%) 2.4560]\n",
      "Whin st sealllyonco-th lin f d athbed t ntouen ane PEThothe d the wome e said w wiurtho wid d we al co \n",
      "\n",
      "[23m 30s (10000 50%) 2.4748]\n",
      "Whe but t t, in she I ll the et.\n",
      "NI be t'garncisoge\n",
      "Woou y\n",
      "O:\n",
      "Fo s in atitour, thom mer sthu I makir'l \n",
      "\n",
      "[23m 44s (10100 50%) 2.5395]\n",
      "Whampithe m,\n",
      "PENThands remeng f linde?\n",
      "\n",
      "KEme sp s l'd s tothavangr thend ho n he w.\n",
      "Ane iugore thinoto \n",
      "\n",
      "[23m 57s (10200 51%) 2.4323]\n",
      "Whixesharst y cak'd astesthe sethawe mallllof ssthe bbecar wull, s d mencade thes w myoutomingand mer  \n",
      "\n",
      "[24m 11s (10300 51%) 2.4814]\n",
      "Wherou himindiel es t n emay the ar itus t hasineangrar ay inoth st lan Ron ferithar n thoonongen Wayo \n",
      "\n",
      "[24m 24s (10400 52%) 2.4643]\n",
      "Whend ale un l heawiofof po wit it st orit tondosteld onos chyouthe pp CHof t chiere s be ve withe f b \n",
      "\n",
      "[24m 38s (10500 52%) 2.5028]\n",
      "Whigorr wand;\n",
      "As sce wot e br I he, it bond thar hen\n",
      "I be ts ton thas am t mand l IS:\n",
      "\n",
      "I cewe s aze y  \n",
      "\n",
      "[24m 51s (10600 53%) 2.4726]\n",
      "Whithe thing mone eest thes chingreinobeathin shak ote?\n",
      "Fl heid hado man f I f of here, che.\n",
      "AR: de, t \n",
      "\n",
      "[25m 4s (10700 53%) 2.4349]\n",
      "Whend, io ta keald bl st INI d an taloune t arrers w s me m me IO:\n",
      "MINI ango s, h an hacorat mathe,\n",
      "S: \n",
      "\n",
      "[25m 18s (10800 54%) 2.5061]\n",
      "Whemathes hire meat te hame the thand d t war:\n",
      "\n",
      "I' f s t me tay hthaionghan t wt hat thede chomy maver \n",
      "\n",
      "[25m 32s (10900 54%) 2.4608]\n",
      "Whachangowirs se r anacouct omyon th makethiomy:\n",
      "I I anone there bu!\n",
      "Her s w telfe he be yo hillestris \n",
      "\n",
      "[25m 45s (11000 55%) 2.4846]\n",
      "Whous,\n",
      "\n",
      "\n",
      "\n",
      "ARary, ISTERI cher,\n",
      "I f ofar\n",
      "AR:\n",
      "Foue ithes ily by thaspechengandese\n",
      "Ang,\n",
      "Therest\n",
      "\n",
      "\n",
      "\n",
      "As nsav \n",
      "\n",
      "[25m 59s (11100 55%) 2.4412]\n",
      "Whofo me cke f amerig, dst igedsatend ay t ryos, the ierer cre l shom theen taitherire r, mendoneat he \n",
      "\n",
      "[26m 12s (11200 56%) 2.4363]\n",
      "Whavicot min as\n",
      "As w the d m ied mer fo renome t,\n",
      "\n",
      "Hafomemar, likns be icr ond, tho.\n",
      "FFINDUCENTI win c \n",
      "\n",
      "[26m 26s (11300 56%) 2.4594]\n",
      "Whe me tan f thind s, may nd t th wathextho bay se.\n",
      "\n",
      "The newhe, man:\n",
      "Therce atide ndomy es ulld th an, \n",
      "\n",
      "[26m 40s (11400 56%) 2.4804]\n",
      "Whann fullal sir sse il tire, whe hithirt s ned n,\n",
      "CA:\n",
      "G s hara ondamy inathine th catourouxertofimak, \n",
      "\n",
      "[26m 54s (11500 57%) 2.5040]\n",
      "Whesee ck he woncan f thave s,\n",
      "Theldelk h:\n",
      "RTherercher we bl sorou he woo hellorthithatid herel tofarb \n",
      "\n",
      "[27m 8s (11600 57%) 2.5492]\n",
      "Whe onous, onoutllam matry sisoure he t.\n",
      "IO:\n",
      "MAng stouno ghout adenor APROMy rd,\n",
      "GLO:\n",
      "Hiler angend s a \n",
      "\n",
      "[27m 22s (11700 58%) 2.4978]\n",
      "Wh, famee macarin e mblkifeeinger ass cd s s sol. tis are my, g whin my her fa intirspofagr no f cen f \n",
      "\n",
      "[27m 36s (11800 59%) 2.4353]\n",
      "Whe if wou s! f Whangr t n t thet owot shine d maveredousthilll issth orens ichile s ms? Cou thav ple  \n",
      "\n",
      "[27m 50s (11900 59%) 2.4581]\n",
      "Whee tillay, s und,\n",
      "G is his andw arandinovene hoff we athesl the bicend be my, al the,\n",
      "Ore athe tr yo \n",
      "\n",
      "[28m 4s (12000 60%) 2.4505]\n",
      "Wh then of s heracother.\n",
      "\n",
      "\n",
      "LO ay d, chis ande?\n",
      "\n",
      "WELLISThe o iot ou, s o ck's' hey hy thit dead suchas  \n",
      "\n",
      "[28m 17s (12100 60%) 2.4825]\n",
      "Whock, bulay hoo myor bof cofofo hes y th thas; y rtomane, ofe mould ove inge llild iffore powaloo war \n",
      "\n",
      "[28m 31s (12200 61%) 2.4785]\n",
      "Whey myous, IORCHe.\n",
      "An.\n",
      "\n",
      "Althe and waro BELETon pe spt pha A:\n",
      "\n",
      "\n",
      "ES:\n",
      "G t nd ys y itoust thothay the se  \n",
      "\n",
      "[28m 45s (12300 61%) 2.4382]\n",
      "Whe wabe hand s to word, he he bler t s, tandema mence d odre as m iver pes herea t y arerd arey at te \n",
      "\n",
      "[28m 59s (12400 62%) 2.4647]\n",
      "Whee pouthate aure s s.\n",
      "Tid MUMy hathioussoul trs tosat set\n",
      "Whar irer the thousthace mu is ttreritsto  \n",
      "\n",
      "[29m 8s (12500 62%) 2.4666]\n",
      "Whavecthon bel My thell l whace it het ghis, t is ato an\n",
      "Pathethe hero s be ister bisthasow I at thert \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94m 42s (12600 63%) 2.4350]\n",
      "Whee, hy t veme:\n",
      "TISeno t, seawinon ond le wo t thange uras s f w tespou hinct hayow lt, hy, sat, sthe \n",
      "\n",
      "[94m 53s (12700 63%) 2.4890]\n",
      "Whe hitugothourisce cha sad wisath lderr ff t chayou matouchis be cis,\n",
      "Ith toter gen go s CERI herald  \n",
      "\n",
      "[94m 59s (12800 64%) 2.4905]\n",
      "Who stathinouny VI mano hisstar inore SCotherifonourt thed.\n",
      "\n",
      "to thiee f t indoutono d t the as s s a t \n",
      "\n",
      "[95m 5s (12900 64%) 2.4342]\n",
      "Whe IOridss akn utowowind bys bre miallord an wates I gu spouss sloule\n",
      "Lore lllavit re;\n",
      "LLAUS:\n",
      "Theer a \n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 80 but got size 79 for tensor number 3 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0e2d8e088fe8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrandom_training_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#train on one chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0mloss_avg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-9691abf743b2>\u001b[0m in \u001b[0;36mrandom_training_set\u001b[1;34m(chunk_len, batch_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrandom_training_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrandom_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_len\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 80 but got size 79 for tensor number 3 in the list."
     ]
    }
   ],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "###Parameters\n",
    "n_epochs = 20000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 5\n",
    "lr = 0.005\n",
    "batch_size = 16\n",
    "chunk_len = 80\n",
    "\n",
    "####\n",
    "\n",
    "model = RNN(n_characters, hidden_size, n_characters, n_layers) #create model\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=lr) #create Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss() #chose criterion\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "\n",
    "def train(inp, target):\n",
    "    \"\"\"\n",
    "    Train sequence for one chunk:\n",
    "    \"\"\"\n",
    "    #reset gradients\n",
    "    model_optimizer.zero_grad() \n",
    "    \n",
    "    # predict output\n",
    "    output = model(inp)\n",
    "    \n",
    "    #compute loss\n",
    "    loss =  criterion(output.view(batch_size*chunk_len,-1), target.view(-1)) \n",
    "\n",
    "    #compute gradients and backpropagate\n",
    "    loss.backward() \n",
    "    model_optimizer.step() \n",
    "\n",
    "    return loss.data.item() \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set(chunk_len,batch_size))  #train on one chunk \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(generate(model,'Wh', 100), '\\n')\n",
    "       \n",
    "\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ec37e",
   "metadata": {},
   "source": [
    "## Visualize loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf419b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab32c42",
   "metadata": {},
   "source": [
    "## Try different temperatures\n",
    "\n",
    "Changing the distribution sharpness has an impact on character sampling:\n",
    "\n",
    "more or less probable things are sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2273e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model,'T', 200, temperature=1))\n",
    "print(\"----\")\n",
    "print(generate(model,'Th', 200, temperature=0.8))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.5))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.3))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc3dea6",
   "metadata": {},
   "source": [
    "### Improving this code:\n",
    "\n",
    "(a) Tinker with parameters:\n",
    "\n",
    "- Is it really necessary to have 100 dims character embeddings\n",
    "- Chunk length can be gradually increased\n",
    "- Try changing RNN cell type (GRUs - LSTMs)\n",
    "\n",
    "(b) Add GPU support to go faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test avec dimension de 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b48332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 50 \n",
    "chunk_len = 40\n",
    "model = RNN(n_characters, hidden_size, n_characters, n_layers,rnn_cell = nn.GRU) #create model\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=lr) #create Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss() #chose criterion\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "\n",
    "def train(inp, target):\n",
    "    \"\"\"\n",
    "    Train sequence for one chunk:\n",
    "    \"\"\"\n",
    "    #reset gradients\n",
    "    model_optimizer.zero_grad() \n",
    "    \n",
    "    # predict output\n",
    "    output = model(inp)\n",
    "    \n",
    "    #compute loss\n",
    "    loss =  criterion(output.view(batch_size*chunk_len,-1), target.view(-1)) \n",
    "\n",
    "    #compute gradients and backpropagate\n",
    "    loss.backward() \n",
    "    model_optimizer.step() \n",
    "\n",
    "    return loss.data.item() \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set(chunk_len,batch_size))  #train on one chunk \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(generate(model,'Wh', 100), '\\n')\n",
    "       \n",
    "\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
