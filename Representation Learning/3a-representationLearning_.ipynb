{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP & representation learning: Neural Embeddings, Text Classification\n",
    "\n",
    "\n",
    "To use statistical classifiers with text, it is first necessary to vectorize the text. In the first practical session we explored the **Bag of Word (BoW)** model. \n",
    "\n",
    "Modern **state of the art** methods uses  embeddings to vectorize the text before classification in order to avoid feature engineering.\n",
    "\n",
    "## [Dataset](https://thome.isir.upmc.fr/classes/RITAL/json_pol)\n",
    "\n",
    "\n",
    "## \"Modern\" NLP pipeline\n",
    "\n",
    "By opposition to the **bag of word** model, in the modern NLP pipeline everything is **embeddings**. Instead of encoding a text as a **sparse vector** of length $D$ (size of feature dictionnary) the goal is to encode the text in a meaningful dense vector of a small size $|e| <<< |D|$. \n",
    "\n",
    "\n",
    "The raw classification pipeline is then the following:\n",
    "\n",
    "```\n",
    "raw text ---|embedding table|-->  vectors --|Neural Net|--> class \n",
    "```\n",
    "\n",
    "\n",
    "### Using a  language model:\n",
    "\n",
    "How to tokenize the text and extract a feature dictionnary is still a manual task. To directly have meaningful embeddings, it is common to use a pre-trained language model such as `word2vec` which we explore in this practical.\n",
    "\n",
    "In this setting, the pipeline becomes the following:\n",
    "```\n",
    "      \n",
    "raw text ---|(pre-trained) Language Model|--> vectors --|classifier (or fine-tuning)|--> class \n",
    "```\n",
    "\n",
    "\n",
    "- #### Classic word embeddings\n",
    "\n",
    " - [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    " - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "- #### bleeding edge language models techniques (see next)\n",
    "\n",
    " - [UMLFIT](https://arxiv.org/abs/1801.06146)\n",
    " - [ELMO](https://arxiv.org/abs/1802.05365)\n",
    " - [GPT](https://blog.openai.com/language-unsupervised/)\n",
    " - [BERT](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Goal of this session:\n",
    "\n",
    "1. Train word embeddings on training dataset\n",
    "2. Tinker with the learnt embeddings and see learnt relations\n",
    "3. Tinker with pre-trained embeddings.\n",
    "4. Use those embeddings for classification\n",
    "5. Compare different embedding models\n",
    "6. Pytorch first look: learn to generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0: Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  \n",
    "warnings.filterwarnings(action='ignore',category=UserWarning,module='gensim')  \n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning,module='gensim') \n",
    "import gensim\n",
    "import logging\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "[\"The undoubted highlight of this movie is Peter O'Toole's performance. In turn wildly comical and terribly terribly tragic. Does anybody do it better than O'Toole? I don't think so. What a great face that man has!<br /><br />The story is an odd one and quite disturbing and emotionally intense in parts (especially toward the end) but it is also oddly touching and does succeed on many levels. However, I felt the film basically revolved around Peter O'Toole's luminous performance and I'm sure I wouldn't have enjoyed it even half as much if he hadn't been in it.\", 1]\n",
      "\n",
      "Number of test reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "['Although credit should have been given to Dr. Seuess for stealing the story-line of \"Horton Hatches The Egg\", this was a fine film. It touched both the emotions and the intellect. Due especially to the incredible performance of seven year old Justin Henry and a script that was sympathetic to each character (and each one\\'s predicament), the thought provoking elements linger long after the tear jerking ones are over. Overall, superior acting from a solid cast, excellent directing, and a very powerful script. The right touches of humor throughout help keep a \"heavy\" subject from becoming tedious or difficult to sit through. Lastly, this film stands the test of time and seems in no way dated, decades after it was released.', 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Loading json\n",
    "with open(\"json_pol\",encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "    json_data = json.loads(data[0])\n",
    "    train = json_data[\"train\"]\n",
    "    test = json_data[\"test\"]\n",
    "    \n",
    "\n",
    "# Quick Check\n",
    "counter_train = Counter((x[1] for x in train))\n",
    "counter_test = Counter((x[1] for x in test))\n",
    "print(\"Number of train reviews : \", len(train))\n",
    "print(\"----> # of positive : \", counter_train[1])\n",
    "print(\"----> # of negative : \", counter_train[0])\n",
    "print(\"\")\n",
    "print(train[0])\n",
    "print(\"\")\n",
    "print(\"Number of test reviews : \",len(test))\n",
    "print(\"----> # of positive : \", counter_test[1])\n",
    "print(\"----> # of negative : \", counter_test[0])\n",
    "\n",
    "print(\"\")\n",
    "print(test[0])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: Quick Recap\n",
    "\n",
    "**[Word2Vec](https://arxiv.org/abs/1301.3781) is composed of two distinct language models (CBOW and SG), optimized to quickly learn word vectors**\n",
    "\n",
    "\n",
    "given a random text: `i'm taking the dog out for a walk`\n",
    "\n",
    "\n",
    "\n",
    "### (a) Continuous Bag of Word (CBOW)\n",
    "    -  predicts a word given a context\n",
    "    \n",
    "maximizing `p(dog | i'm taking the ___ out for a walk)`\n",
    "    \n",
    "### (b) Skip-Gram (SG)               \n",
    "    -  predicts a context given a word\n",
    "    \n",
    " maximizing `p(i'm taking the out for a walk | dog)`\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: train a language model (word2vec)\n",
    "\n",
    "Gensim has one of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) fastest implementation.\n",
    "\n",
    "\n",
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gensim not installed yet\n",
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "text = [t.split() for t,p in train]\n",
    "\n",
    "# the following configuration is the default configuration\n",
    "w2v = gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                vector_size=100, window=5,               ### here we train a cbow model \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worth it to save the previous embedding\n",
    "# w2v.save(\"W2v-movies.dat\")\n",
    "# You will be able to reload them:\n",
    "w2v = gensim.models.Word2Vec.load(\"W2v-movies.dat\")\n",
    "# and you can continue the learning process if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Test learnt embeddings\n",
    "\n",
    "The word embedding space directly encodes similarities between words: the vector coding for the word \"great\" will be closer to the vector coding for \"good\" than to the one coding for \"bad\". Generally, [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is the distance used when considering distance between vectors.\n",
    "\n",
    "KeyedVectors have a built in [similarity](https://radimrehurek.com/gensim/models /keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.similarity) method to compute the cosine similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great and good: 0.7798576\n",
      "great and bad: 0.45752424\n"
     ]
    }
   ],
   "source": [
    "# is great really closer to good than to bad ?\n",
    "print(\"great and good:\",w2v.wv.similarity(\"great\",\"good\"))\n",
    "print(\"great and bad:\",w2v.wv.similarity(\"great\",\"bad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cosine distance encodes similarity, neighboring words are supposed to be similar. The [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.most_similar) method returns the `topn` words given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Though', 0.863980770111084),\n",
       " ('While', 0.8636260032653809),\n",
       " ('However,', 0.8314102292060852),\n",
       " ('However', 0.803390383720398),\n",
       " ('Still,', 0.795049786567688)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The query can be as simple as a word, such as \"movie\"\n",
    "\n",
    "# Try changing the word\n",
    "# w2v.wv.most_similar(\"movie\",topn=5) # 5 most similar words\n",
    "# w2v.wv.most_similar(\"awesome\",topn=5)\n",
    "w2v.wv.most_similar(\"Although\",topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can be a more complicated query\n",
    "Word embedding spaces tend to encode much more.\n",
    "\n",
    "The most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"film\"', 0.7406459450721741),\n",
       " ('it', 0.6212536692619324),\n",
       " ('movie,', 0.6170195937156677)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is awesome - good + bad ?\n",
    "w2v.wv.most_similar(positive=[\"awesome\",\"bad\"],negative=[\"good\"],topn=3)  \n",
    "\n",
    "w2v.wv.most_similar(positive=[\"actor\",\"woman\"],negative=[\"man\"],topn=3) # do the famous exemple works for actor ?\n",
    "w2v.wv.most_similar(positive=[\"movie\",\"film\"],negative=[\"movies\"],topn=3)\n",
    "\n",
    "# Try other things like plurals for exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test learnt \"synctactic\" and \"semantic\" similarities, Mikolov et al. introduced a special dataset containing a wide variety of three way similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = w2v.wv.evaluate_word_analogies(\"questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When training the w2v models on the review dataset, since it hasn't been learnt with a lot of data, it does not perform very well.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Loading a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gensim, embeddings are loaded and can be used via the [\"KeyedVectors\"](https://radimrehurek.com/gensim/models/keyedvectors.html) class\n",
    "\n",
    "> Since trained word vectors are independent from the way they were trained (Word2Vec, FastText, WordRank, VarEmbed etc), they can be represented by a standalone structure, as implemented in this module.\n",
    "\n",
    ">The structure is called “KeyedVectors” and is essentially a mapping between entities and vectors. Each entity is identified by its string id, so this is a mapping between {str => 1D numpy array}.\n",
    "\n",
    ">The entity typically corresponds to a word (so the mapping maps words to 1D vectors), but for some models, they key can also correspond to a document, a graph node etc. To generalize over different use-cases, this module calls the keys entities. Each entity is always represented by its string id, no matter whether the entity is a word, a document or a graph node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from gensim.test.utils import get_tmpfile\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "bload = True\n",
    "fname = \"word2vec-google-news-300\"\n",
    "sdir = \"word2vec-google-news-300/\"\n",
    "\n",
    "if(bload==True):\n",
    "    wv_pre_trained = KeyedVectors.load(sdir+fname+\".dat\")\n",
    "else:    \n",
    "    wv_pre_trained = api.load(fname)\n",
    "    wv_pre_trained.save(sdir+fname+\".dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = wv_pre_trained.evaluate_word_analogies(\"questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform the \"synctactic\" and \"semantic\" evaluations again. Conclude on the pre-trained embeddings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4:  sentiment classification\n",
    "\n",
    "In the previous practical session, we used a bag of word approach to transform text into vectors.\n",
    "Here, we propose to try to use word vectors (previously learnt or loaded).\n",
    "\n",
    "\n",
    "### <font color='green'> Since we have only word vectors and that sentences are made of multiple words, we need to aggregate them. </font>\n",
    "\n",
    "\n",
    "### (1) Vectorize reviews using word vectors:\n",
    "\n",
    "Word aggregation can be done in different ways:\n",
    "\n",
    "- Sum\n",
    "- Average\n",
    "- Min/feature\n",
    "- Max/feature\n",
    "\n",
    "#### a few pointers:\n",
    "\n",
    "- `w2v.wv.vocab` is a `set()` of the vocabulary (all existing words in your model)\n",
    "- `np.minimum(a,b) and np.maximum(a,b)` respectively return element-wise min/max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00363242  0.12418909  0.01566555  0.04415126 -0.00508231 -0.27246916\n",
      "  0.19513953  0.3751213  -0.34567207 -0.26771784 -0.01835521 -0.3196532\n",
      "  0.0329181   0.20886584  0.12046224 -0.11845102  0.10777609 -0.12048113\n",
      " -0.03360869 -0.4484004   0.15153204  0.07441775  0.16307026 -0.24295951\n",
      "  0.11419878 -0.04394715 -0.14721656  0.03436415 -0.1520337   0.21216539\n",
      "  0.29205474 -0.11554233  0.11436694 -0.23363574 -0.04453686  0.1854144\n",
      " -0.02902844  0.07646015 -0.16564287 -0.16695191  0.2006799  -0.10824912\n",
      " -0.1035899   0.0131361   0.13083714 -0.01996426 -0.21404062 -0.08637805\n",
      "  0.25914088  0.23668924  0.107595   -0.2512329  -0.01381236  0.04372877\n",
      "  0.01957311  0.11562493  0.07492033  0.11477908 -0.17324945  0.08195102\n",
      " -0.10190435 -0.02096242  0.2491143   0.11524353 -0.11877654  0.21503681\n",
      "  0.01073598  0.1223154  -0.19175617  0.15435275  0.06035062  0.12463818\n",
      "  0.13395752  0.03213201  0.1438389  -0.07004435  0.18406384 -0.05245426\n",
      " -0.07198591  0.00593548 -0.24222563 -0.09089641 -0.18064833  0.19170289\n",
      " -0.08802183 -0.04518391  0.15265213  0.17058426  0.2271863   0.17071041\n",
      "  0.37750095  0.00379824  0.0114666  -0.06349201  0.24398455 -0.09189673\n",
      "  0.02961702 -0.17750128 -0.04825141 -0.07500256]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# We first need to vectorize text:\n",
    "# First we propose to a sum of them\n",
    "\n",
    "\n",
    "def vectorize(text,model,mean=False,minFeat=False,maxFeat=False):\n",
    "    '''\n",
    "    This function should vectorize one review\n",
    "\n",
    "    input: str\n",
    "    output: np.array(float)\n",
    "    '''\n",
    "    vocabulary = model.wv.key_to_index.keys()\n",
    "    \n",
    "    weights = [model.wv.get_vector(word) for word in text.split(\" \") if word in vocabulary]\n",
    "    \n",
    "    if mean:\n",
    "        return np.mean(weights,axis=0)\n",
    "    if minFeat:\n",
    "        return np.min(weights,axis=0)\n",
    "    if maxFeat:\n",
    "        return np.max(weights,axis=0)\n",
    "    \n",
    "    return np.sum(weights,axis=0)\n",
    "    \n",
    "classes = [pol for text,pol in train]\n",
    "X = [vectorize(text,w2v,mean=True) for text,pol in train]\n",
    "X_test = [vectorize(text,w2v,mean=True) for text,pol in test]\n",
    "true = [pol for text,pol in test]\n",
    "\n",
    "#let's see what a review vector looks like.\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amine/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "/home/amine/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# t-SNE from the U matrix computed by LSA\n",
    "tsne = TSNE(n_components=2, init='pca',n_iter=250, verbose=0)\n",
    "tsne_mat = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f179c5aaf40>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cla1 = np.where(np.array(classes) == 0)\n",
    "cla2 = np.where(np.array(classes) == 1)\n",
    "plt.scatter(tsne_mat[cla1,0],tsne_mat[cla1,1],color='Red')\n",
    "plt.scatter(tsne_mat[cla2,0],tsne_mat[cla2,1],color='Green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Train a classifier \n",
    "as in the previous practical session, train a logistic regression to do sentiment classification with word vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.8204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amine/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,classes, test_size=0.2, random_state=0) \n",
    "\n",
    "lr_clf = LogisticRegression(random_state=0, solver='lbfgs',n_jobs=-1)\n",
    "lr_clf.fit(X_train,y_train)\n",
    "pred_lr = lr_clf.predict(X_test)\n",
    "print(f\"Logistic Regression accuracy: {accuracy_score(y_test, pred_lr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance should be worst than with bag of word (~80%). Sum/Mean aggregation does not work well on long reviews (especially with many frequent words). This adds a lot of noise.\n",
    "\n",
    "## **Todo** :  Try answering the following questions:\n",
    "\n",
    "- Which word2vec model works best: skip-gram or cbow\n",
    "- Do pretrained vectors work best than those learnt on the train dataset ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(Bonus)** To have a better accuracy, we could try two things:\n",
    "- Better aggregation methods (weight by tf-idf ?)\n",
    "- Another word vectorizing method such as [fasttext](https://radimrehurek.com/gensim/models/fasttext.html)\n",
    "- A document vectorizing method such as [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregation avec ponderation de tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(train)\n",
    "# corpus = \" \".join(data[:1000,0])\n",
    "corpus = data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The undoubted highlight of this movie is Peter O'Toole's performance. In turn wildly comical and terribly terribly tragic. Does anybody do it better than O'Toole? I don't think so. What a great face that man has!<br /><br />The story is an odd one and quite disturbing and emotionally intense in parts (especially toward the end) but it is also oddly touching and does succeed on many levels. However, I felt the film basically revolved around Peter O'Toole's luminous performance and I'm sure I wouldn't have enjoyed it even half as much if he hadn't been in it.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf= True, smooth_idf=True, sublinear_tf=True)\n",
    "bow_tfidf = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23646594157738154"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = bow_tfidf.getrow(0).toarray()\n",
    "# bow_tfidf.get\n",
    "# vovalues = np.array(list(w2v.wv.key_to_index.keys()))\n",
    "# words = np.array(vectorizer.get_feature_names())\n",
    "# same = np.intersect1d(words,vovalues)\n",
    "# # w00 = same[1020]\n",
    "# # w2v.wv.get_vector(w)\n",
    "# new_w2v = []\n",
    "# doc0 = corpus[0]\n",
    "# for word in doc0.split(\" \"):\n",
    "#     if word in same:\n",
    "#         print(bow_tfidf[0])\n",
    "\n",
    "def get_tfidf_word(word,num_document,vectorizer,bow_tfidf):\n",
    "    words = np.array(vectorizer.get_feature_names())\n",
    "    indice = int(np.where(words == word)[0])\n",
    "    tfidf_docu = bow_tfidf.getrow(num_document).toarray()\n",
    "    # tfidf_docu est un vecteur colonne \n",
    "    return tfidf_docu[:,indice][0]\n",
    "\n",
    "get_tfidf_word(\"undoubted\",0,vectorizer,bow_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The undoubted highlight of this movie is Peter O'Toole's performance. In turn wildly comical and terribly terribly tragic. Does anybody do it better than O'Toole? I don't think so. What a great face that man has!<br /><br />The story is an odd one and quite disturbing and emotionally intense in parts (especially toward the end) but it is also oddly touching and does succeed on many levels. However, I felt the film basically revolved around Peter O'Toole's luminous performance and I'm sure I wouldn't have enjoyed it even half as much if he hadn't been in it.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la moyenne de TfIdf(doc,word) * w2v(word)\n",
    "def vectorize_tfidf(text,num_document,bow_tfidf,vectorizer):\n",
    "    \n",
    "    \n",
    "    vovalues = np.array(list(w2v.wv.key_to_index.keys()))\n",
    "    words = np.array(vectorizer.get_feature_names())\n",
    "    same = np.intersect1d(words,vovalues)\n",
    "    vocabulary = w2v.wv.key_to_index.keys()\n",
    "    \n",
    "    weights = []\n",
    "    for word in text.split(\" \"):\n",
    "        if word in same:\n",
    "            ponderation = get_tfidf_word(word,num_document,vectorizer,bow_tfidf)\n",
    "        else:\n",
    "            ponderation = 1\n",
    "        # si c'est un mot commun entre le vectorizer et l'ensemble des word2vec \n",
    "        # alors les poids serront le vecteur w2v(mot) * tf_idf(du_mot)\n",
    "        # sinon w2v(mot) lui meme : w2v(mot) * 1\n",
    "        if word in vocabulary: weights.append(ponderation * w2v.wv.get_vector(word))\n",
    "            \n",
    "    return np.sum(weights,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amine/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_tfidf = [vectorize_tfidf(train[i][0],i,bow_tfidf,vectorizer) for i in range(len(train))]\n",
    "X_tfidf_test = [vectorize_tfidf(test[i][0],i,bow_tfidf,vectorizer) for i in range(len(test))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basé sur une representation n-gramme en caractere pour un mot donné.\n",
    "- Ainsi un mot serra une somme de vecteurs de n-gramme caracteres qui compose ce mot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "\n",
    "text = [t.split() for t,p in train]\n",
    "\n",
    "model = FastText(sentences=text,\n",
    "                                vector_size=100, window=5,               ### here we train a cbow model \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1, epochs=5)  # instantiate\n",
    "model.build_vocab(corpus_iterable=text)\n",
    "model.train(corpus_iterable=common_texts, total_examples=len(text), epochs=10)  # train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Difference entre vecteur de word2vec et FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16731083 -0.19329427 -0.09280075 -0.43239114 -0.07779495 -0.04488283\n",
      "  0.34829816 -0.05742278 -0.02294259  0.01026742 -0.09949178  0.1954108\n",
      " -0.09347434  0.5329962   0.05762577 -0.15057018 -0.20509721 -0.30254737\n",
      " -0.4253131  -0.3018075  -0.38061884 -0.24797621 -0.11755973  0.04720357\n",
      " -0.21223906  0.06079886 -0.30239588 -0.10445249 -0.34980914  0.16825941\n",
      " -0.29910317 -0.07196746  0.18992278 -0.08510811  0.3167359   0.30600885\n",
      " -0.10006637 -0.2099046  -0.19125764 -0.19242759  0.4487      0.17721424\n",
      "  0.15086827 -0.22347932 -0.11607855  0.2218838  -0.09441052  0.07214619\n",
      "  0.16597748 -0.3014298   0.33025903 -0.05773444  0.44015136 -0.43615627\n",
      " -0.0497252  -0.11084421  0.16228652  0.14747818  0.11073399  0.18798974\n",
      " -0.31871694 -0.49047238  0.36337087 -0.06827755 -0.02935816  0.3051142\n",
      "  0.18078311  0.14340688  0.24597026  0.02221462  0.1350785   0.18532106\n",
      "  0.03584741 -0.44702083  0.05997883  0.40351677  0.21511497  0.08850449\n",
      "  0.0942331   0.1752117   0.28343043 -0.3713971  -0.31197256 -0.38460034\n",
      "  0.06025194 -0.32623577  0.4665124   0.0576834   0.07769816  0.09587926\n",
      "  0.38827345 -0.08209603  0.32540318  0.22794938 -0.05885887 -0.15915307\n",
      " -0.05144442 -0.05377293  0.24970068 -0.46053523]\n",
      "\n",
      "[-0.12901239  0.09364843  0.03368481  0.19551845  0.04115085 -0.19658314\n",
      "  0.07625866  0.389226   -0.3515046  -0.08956046 -0.05181596 -0.6037916\n",
      "  0.02712612  0.330458   -0.1166629  -0.36718744  0.20514882 -0.18072514\n",
      " -0.0929803  -0.54061955  0.3346625  -0.13612059  0.20070943 -0.09342517\n",
      "  0.1111997   0.11219504 -0.2970621   0.31009996 -0.23387235 -0.03202559\n",
      "  0.06372932 -0.2817556   0.07840063 -0.2707108  -0.04579486 -0.09302532\n",
      "  0.05835734 -0.17524129 -0.20556022 -0.04897486  0.13371591 -0.04252703\n",
      " -0.05258852 -0.00820026  0.10442372 -0.23256832 -0.3679684   0.07082546\n",
      "  0.36207172  0.49790764  0.00116705 -0.09233285 -0.12665945 -0.01753704\n",
      "  0.10227124 -0.10833732  0.03000843 -0.04831162 -0.19179396 -0.04798803\n",
      " -0.11168536 -0.10920144  0.21418124  0.13298157 -0.03228002 -0.003253\n",
      " -0.10513738  0.36963034 -0.10581998  0.31438527  0.18776393  0.12294347\n",
      "  0.01847838  0.17901748  0.17464781 -0.06205272  0.18018751  0.44374368\n",
      " -0.08090674 -0.17134441 -0.01009121 -0.07506572 -0.30564332  0.2130291\n",
      " -0.07325067  0.03749077  0.4002214   0.10151462  0.04892904  0.3102583\n",
      "  0.2618324  -0.08056212  0.13915879 -0.4376486   0.4401592  -0.0059097\n",
      " -0.07959761 -0.21352832  0.25225484  0.00888331]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.get_vector('the'))\n",
    "print()\n",
    "print(w2v.wv.get_vector('the'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparaison de performance entre word2vec et FastText pour une agregation de somme par exemple pour chaque document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft = [vectorize(text,model,mean=True) for text,pol in train]\n",
    "X_ft_test = [vectorize(text,model,mean=True) for text,pol in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.8138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amine/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_ft,classes, test_size=0.2, random_state=0) \n",
    "\n",
    "lr_clf = LogisticRegression(random_state=0, solver='lbfgs',n_jobs=-1)\n",
    "lr_clf.fit(X_train,y_train)\n",
    "pred_lr = lr_clf.predict(X_test)\n",
    "print(f\"Logistic Regression accuracy: {accuracy_score(y_test, pred_lr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "d2v = Doc2Vec(corpus, vector_size=5, window=2, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
